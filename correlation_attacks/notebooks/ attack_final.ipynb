{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3eb869d",
   "metadata": {},
   "source": [
    "# Application of correlation attacks: column-wise, row-wise, and integrated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db2a63",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to embed a universal fingerprint into the Adult dataset, then conduct and evaluate 3 types of attacks: column-wise, row-wise, and their integration, by measuring how many fingerprint bits each attack corrupts (robustness) and how much data utility is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7801bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_root = Path().resolve().parents[1]\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "results_dir = Path().resolve().parents[1] / 'correlation_attacks' / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scheme._universal import Universal\n",
    "import importlib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gensim.downloader as api\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import correlation_attacks.src.attacks2 as attacks2\n",
    "\n",
    "importlib.reload(attacks2)\n",
    "\n",
    "from correlation_attacks.src.attacks2 import (\n",
    "    compute_joint_distribution,\n",
    "    column_wise_attack,\n",
    "    compute_row_similarity,\n",
    "    row_wise_attack,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df41be",
   "metadata": {},
   "source": [
    "### Preparation (incl. Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff649b0c",
   "metadata": {},
   "source": [
    "Loading and preprocessesing the Adult Census data, semantically encoding “education” and “marital‐status”,  factorizing other categoricals, and binning numeric columns, then embeds a universal fingerprint.\n",
    "Also here we compute pairwise joint distributions for column‐wise attacks and K‐means communities with their exponential Hamming‐based row similarities for row‐wise attacks, and also set the column and row attack thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f1e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding fingerprint\n",
      "Universal fingerprinting scheme - initialised.\n",
      "Embedding started...\n",
      "\tgamma: 35\n",
      "\tfingerprint length: 128\n",
      "\n",
      "Generated fingerprint for recipient 7: 01010011001110010101101110011111100110001000011111010111000100001101000001011010110000001111010110101011010101101110111101100110\n",
      "Fingerprint inserted.\n",
      "\tmarked tuples: ~2.95%\n",
      "\tsingle fingerprint bit embedded 7 times (\"amount of redundancy\")\n",
      "Time: <1 sec.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967f63ec243e466394a34f78dfbcc0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Joint dists: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building communities via KMeans\n",
      "Average cluster size: 3975.48908203065\n",
      "Computing true row similarities\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f68093ed6b34032a91b361b9cbfdf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Row sims:   0%|          | 0/32561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/adult_train_id.csv').iloc[:,1:]\n",
    "#df = df.iloc[:1000].copy() # to test a smaller subset\n",
    "wv = api.load('glove-wiki-gigaword-100')  \n",
    "\n",
    "def embed_label(label: str):\n",
    "    toks = label.replace('-', ' ').split()\n",
    "    vecs = [wv[t] for t in toks if t in wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(wv.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def make_semantic_codes(series: pd.Series) -> pd.Series:\n",
    "    cats = series.unique().tolist()\n",
    "    embeds = np.vstack([embed_label(c) for c in cats])\n",
    "    Z = linkage(embeds, method='average', metric='euclidean')\n",
    "    order = leaves_list(Z)\n",
    "    ordered = [cats[i] for i in order]\n",
    "    code_map = {cat: idx for idx, cat in enumerate(ordered)}\n",
    "    return series.map(code_map)\n",
    "\n",
    "df['education']      = make_semantic_codes(df['education'])\n",
    "df['marital-status'] = make_semantic_codes(df['marital-status'])\n",
    "\n",
    "nominals = ['workclass','occupation','relationship','race','sex','native-country','income']\n",
    "for col in nominals:\n",
    "    df[col], _ = pd.factorize(df[col], sort=True)\n",
    "\n",
    "age_bins = [17, 26, 36, 46, 56, 66, df['age'].max() + 1]\n",
    "df['age'] = pd.cut(\n",
    "    df['age'],\n",
    "    bins=age_bins,\n",
    "    right=False,     # intervals are [17,26), [26,36)\n",
    "    labels=False\n",
    ").astype(int)\n",
    "\n",
    "# hours-per-week: [0–20, 21–40, 41–60, 61–80, 81+]\n",
    "hpw_bins = [0, 21, 41, 61, 81, df['hours-per-week'].max() + 1]\n",
    "df['hours-per-week'] = pd.cut(\n",
    "    df['hours-per-week'],\n",
    "    bins=hpw_bins,\n",
    "    right=False,\n",
    "    labels=False\n",
    ").astype(int)\n",
    "\n",
    "# capital-gain and capital-loss\n",
    "for col in ['capital-gain', 'capital-loss']:\n",
    "    df[col] = (\n",
    "        pd.qcut(df[col].rank(method='first'),\n",
    "                q=4,\n",
    "                labels=False,\n",
    "                duplicates='drop')\n",
    "          .astype(int)\n",
    "    )\n",
    "\n",
    "# fnlwgt into 5 quantile bins\n",
    "df['fnlwgt'] = (\n",
    "    pd.qcut(df['fnlwgt'].rank(method='first'),\n",
    "            q=5,\n",
    "            labels=False,\n",
    "            duplicates='drop')\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "# final attribute list\n",
    "attrs = df.columns.tolist()\n",
    "#df_preprocessed.to_csv('adult_preprocessed.csv', index=False)\n",
    "\n",
    "# embedding universal fingerprint\n",
    "u = Universal(gamma=35, fingerprint_bit_length=128, xi=1)\n",
    "print(\"Embedding fingerprint\")\n",
    "df_fp = u.insertion(\n",
    "    dataset=df,\n",
    "    recipient_id=7,\n",
    "    secret_key=42\n",
    ")\n",
    "\n",
    "true_joint = {}\n",
    "for p, q in tqdm(combinations(attrs, 2), desc=\"Joint dists\"):\n",
    "    true_joint[(p, q)] = compute_joint_distribution(df, p, q)\n",
    "\n",
    "\n",
    "print(\"Building communities via KMeans\")\n",
    "X = df[attrs].values\n",
    "kmeans = KMeans(n_clusters=10, random_state=42).fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "clusters = {}\n",
    "for c in range(10):\n",
    "    members = np.where(labels == c)[0].tolist()\n",
    "    for i in members:\n",
    "        clusters[i] = [j for j in members if j != i]\n",
    "\n",
    "print(\"Average cluster size:\", \n",
    "      np.mean([len(members) for members in clusters.values()]))\n",
    "\n",
    "print(\"Computing true row similarities\")\n",
    "true_sims = {}\n",
    "for i, members in tqdm(clusters.items(), desc=\"Row sims\"):\n",
    "    for j in members:\n",
    "        # raw Hamming distance\n",
    "        dist_ij = (df.iloc[i] != df.iloc[j]).sum()\n",
    "        sim = np.exp(-dist_ij)\n",
    "        true_sims[(i, j)] = sim\n",
    "        true_sims[(j, i)] = sim\n",
    "\n",
    "\n",
    "tau_col = 1e-4\n",
    "tau_row = 0.1\n",
    "print(f\"Using thresholds tau_col={tau_col}, tau_row={tau_row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e19c7",
   "metadata": {},
   "source": [
    "### Running the attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the attacks\n",
    "\n",
    "def extract_df(obj):\n",
    "    return obj.dataframe if hasattr(obj, 'dataframe') else obj\n",
    "\n",
    "print(\"Running column-wise attack\")\n",
    "df_col = column_wise_attack(\n",
    "    extract_df(df_fp).copy(),   \n",
    "    true_joint,\n",
    "    threshold=tau_col,\n",
    "    pbar=tqdm\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Running row-wise attack\")\n",
    "df_row = row_wise_attack(\n",
    "    extract_df(df_fp).copy(),\n",
    "    clusters,\n",
    "    true_sims,\n",
    "    threshold=tau_row,\n",
    "    pbar=tqdm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrated wrapper\n",
    "def integrated_attack(df_input, true_joint, clusters, true_sims, tau_row, tau_col, iters=1, pbar=None):\n",
    "    df_curr = df_input\n",
    "    for _ in range(iters):\n",
    "        df_curr = row_wise_attack(df_curr, clusters, true_sims, threshold=tau_row, pbar=pbar)\n",
    "        df_curr = column_wise_attack(df_curr, true_joint, threshold=tau_col, pbar=pbar)\n",
    "    return df_curr\n",
    "\n",
    "print(\"Running integrated attack\")\n",
    "df_int = integrated_attack(extract_df(df_fp).copy(), true_joint, clusters, true_sims, tau_row, tau_col, iters=1, pbar=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2977c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving datasets\n",
    "def extract_df(obj): return obj.dataframe if hasattr(obj,'dataframe') else obj\n",
    "\n",
    "extract_df(df_col).to_csv(results_dir / 'adult_col_attacked2.csv', index=False)\n",
    "extract_df(df_row).to_csv(results_dir / 'adult_row_attacked2.csv', index=False)\n",
    "extract_df(df_int).to_csv(results_dir / 'adult_integrated_attacked2.csv', index=False)\n",
    "\n",
    "print(f\"Saved attacked datasets to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7514cd4",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0e262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_col = pd.read_csv('../results/adult_col_attacked2.csv')\n",
    "df_row = pd.read_csv('../results/adult_row_attacked2.csv')\n",
    "df_int = pd.read_csv('../results/adult_integrated_attacked2.csv')\n",
    "\n",
    "\n",
    "def extract(a):\n",
    "    return a.dataframe if hasattr(a, 'dataframe') else a\n",
    "\n",
    "# robustness\n",
    "def eval_match(a):\n",
    "    orig_fp = u.create_fingerprint(recipient_id=7, secret_key=42)\n",
    "    _ = u.detection(dataset=a, secret_key=42)\n",
    "    cnts = u.detection_counts\n",
    "    rec_bits = []\n",
    "    for c0, c1 in cnts:\n",
    "        if c1 > c0:\n",
    "            rec_bits.append('1')\n",
    "        elif c0 > c1:\n",
    "            rec_bits.append('0')\n",
    "        else:\n",
    "            rec_bits.append('2') \n",
    "    rec_fp = ''.join(rec_bits)\n",
    "    match = sum(o==r for o,r in zip(orig_fp,rec_fp)) / len(orig_fp)\n",
    "    numcmp= sum(o!=r for o,r in zip(orig_fp,rec_fp))\n",
    "    return match, numcmp\n",
    "\n",
    "print(\"Robustness (match_rate, #compromised):\")\n",
    "for name, df_a in [('Column-wise',df_col),('Row-wise',df_row),('Integrated',df_int)]:\n",
    "    print(f\"{name}: {eval_match(df_a)}\")\n",
    "\n",
    "# utility\n",
    "orig_df = df\n",
    "print(\"\\nUtility metrics:\")\n",
    "for name, atk in [('Column-wise',df_col),('Row-wise',df_row),('Integrated',df_int)]:\n",
    "    d = extract(atk)\n",
    "    # Acc\n",
    "    Acc = 1 - (orig_df.values!=d.values).sum()/orig_df.size\n",
    "\n",
    "    # Pcol: fraction of joint-cells where |J_emp-J_true| <= τ_col\n",
    "    total_cells = 0; ok_cells = 0\n",
    "    for (p,q), J_true in true_joint.items():\n",
    "        J_emp = compute_joint_distribution(d,p,q).reindex(\n",
    "                    index=J_true.index, columns=J_true.columns, fill_value=0)\n",
    "        diffs = np.abs(J_emp.values - J_true.values)\n",
    "        total_cells += diffs.size\n",
    "        ok_cells    += (diffs <= tau_col).sum()\n",
    "    Pcol = ok_cells/total_cells\n",
    "\n",
    "    # Prow: fraction of row-pairs in communities where |s_emp-s_true| <= τ_row\n",
    "    total_pairs = 0; ok_pairs = 0\n",
    "    for i, members in clusters.items():\n",
    "        for j in members:\n",
    "            if i < j:\n",
    "                # recompute s_emp\n",
    "                dist_emp = (d.iloc[i] != d.iloc[j]).sum()\n",
    "                s_emp     = np.exp(-dist_emp)\n",
    "                total_pairs += 1\n",
    "                if abs(s_emp - true_sims[(i,j)]) <= tau_row:\n",
    "                    ok_pairs += 1\n",
    "    Prow = ok_pairs/total_pairs\n",
    "\n",
    "    # Pcov\n",
    "    cov0 = np.cov(orig_df.values, rowvar=False)\n",
    "    cov1 = np.cov(d.values,       rowvar=False)\n",
    "    Pcov = 1 - norm(cov0-cov1,'fro')/norm(cov0,'fro')\n",
    "\n",
    "    print(f\"{name}: Acc={Acc:.3%}, Pcol={Pcol:.3%}, Prow={Prow:.3%}, Pcov={Pcov:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36547865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import filecmp\n",
    "if filecmp.cmp('../results/adult_col_attacked2.csv', '../results/adult_integrated_attacked2.csv', shallow=False):\n",
    "    print(\"Files are identical (byte-for-byte).\")\n",
    "else:\n",
    "    print(\"Files differ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97371aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pipenv)",
   "language": "python",
   "name": "pipenv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
